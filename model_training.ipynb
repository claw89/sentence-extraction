{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "5AgEdA8oAZNT",
    "outputId": "f132a1bf-56e7-4c90-a8cf-1b05a7021ad7"
   },

   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BdmnRuXaFI5x"
   },
   "outputs": [],
   "source": [
    "from transformers import (T5Tokenizer, \n",
    "                          T5ForConditionalGeneration, \n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9bJallwRPhi"
   },
   "source": [
    "Try this training function from https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAe4QEELRxJs"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import editdistance\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xT_pgJeeX6G"
   },
   "source": [
    "Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "colab_type": "code",
    "id": "pzk5-4rtekni",
    "outputId": "2ace28c8-1e21-4309-b19b-86815b70e5e6"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sentences.tsv\", \"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w58W-Rd5jbv1"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "94MG7ACS_TVa",
    "outputId": "103d1bcf-29c2-48c2-e0f8-cf6cef091dc4"
   },

   "outputs": [],
   "source": [
    "df.original.str.len().hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_vSEB5Q_1uA"
   },
   "outputs": [],
   "source": [
    "df = df[df.original.str.len() < 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vaKCsSOYGduw"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACTjFBKTQt2x"
   },
   "outputs": [],
   "source": [
    "df[\"edit_distance\"] = df.apply(lambda x: editdistance.eval(x[0], x[1]) / max(len(x[0]), len(x[1])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "3ElrO_4oRlWT",
    "outputId": "fc60147b-d536-49bf-9cae-0bf7447b9810"
   },
   "outputs": [],
   "source": [
    "df.edit_distance.hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SpCgLIu3wmbb"
   },
   "outputs": [],
   "source": [
    "df.original = pd.Series([\"edit: \"] * df.shape[0]) + df.original\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvJ5avbgJdn4"
   },
   "outputs": [],
   "source": [
    "class EditDataset(Dataset):\n",
    "    def __init__(self, original_sentences, revised_sentences):\n",
    "        self.original_sentences = original_sentences\n",
    "        self.revised_sentences = revised_sentences\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.revised_sentences))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (self.original_sentences[i], self.revised_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elM3cRAyKM5K"
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader):\n",
    "  running_loss = 0.\n",
    "  epoch_loss = 0.\n",
    "  start_time = time.time()\n",
    "  for batch, (data, target) in enumerate(trainloader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    max_length_data = max([len(tokenizer.encode(sent)) for sent in data])\n",
    "    data = torch.Tensor([tokenizer.encode(sent, max_length=max_length_data, pad_to_max_length=True) for sent in data])\n",
    "    data = data.to(torch.int64)\n",
    "    max_length_target = max([len(tokenizer.encode(sent)) for sent in target])\n",
    "    target = torch.Tensor([tokenizer.encode(sent, max_length=max_length_target, pad_to_max_length=True) for sent in target])\n",
    "    target = target.to(torch.int64)\n",
    "    \n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    outputs = model(input_ids=data, lm_labels=target)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    #Reuse GPU memory\n",
    "    del data\n",
    "    del target\n",
    "    torch.cuda.empty_cache()\n",
    "    ##########################\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    epoch_loss += loss.item()\n",
    "    log_interval = 200\n",
    "    if batch % log_interval == 0 and batch > 0:\n",
    "      cur_loss = running_loss / log_interval\n",
    "      elapsed = time.time() - start_time\n",
    "      print('| epoch {:3d} | batch {:5d} / {:5d} | '\n",
    "            'lr {:05.5f} | ms/batch {:5.2f} | '\n",
    "            'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "              epoch + 1, batch, len(trainloader),\n",
    "              scheduler.get_last_lr()[0],\n",
    "              elapsed * 1000 / log_interval,\n",
    "              cur_loss, math.exp(cur_loss)))\n",
    "      running_loss = 0.\n",
    "      start_time = time.time()\n",
    "  \n",
    "  ## Validation\n",
    "  validation_loss = 0.\n",
    "  for batch, (data, target) in enumerate(valloader):\n",
    "    max_length_data = max([len(tokenizer.encode(sent)) for sent in data])\n",
    "    data = torch.Tensor([tokenizer.encode(sent, max_length=max_length_data, pad_to_max_length=True) for sent in data])\n",
    "    data = data.to(torch.int64)\n",
    "    \n",
    "    max_length_target = max([len(tokenizer.encode(sent)) for sent in target])\n",
    "    target = torch.Tensor([tokenizer.encode(sent, max_length=max_length_target, pad_to_max_length=True) for sent in target])\n",
    "    target = target.to(torch.int64)\n",
    "    \n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    outputs = model(input_ids=data, lm_labels=target)\n",
    "    loss = outputs[0]\n",
    "    validation_loss += loss.item()\n",
    "\n",
    "    #Reuse GPU memory\n",
    "    del data\n",
    "    del target\n",
    "    torch.cuda.empty_cache()\n",
    "    ##########################\n",
    "    \n",
    "  epoch_loss = epoch_loss/len(trainloader)\n",
    "  validation_loss = validation_loss/len(valloader)\n",
    "  print('############# Epoch {:3d} ##############'.format(epoch + 1))\n",
    "  print('Train loss: {:5.2f}; Validation loss: {:5.2f}'.format(epoch_loss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doVCjJpPnbP0"
   },
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df[df.edit_distance > 0.3], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDhCkdYhe5JH"
   },
   "outputs": [],
   "source": [
    "train_data = EditDataset(df_train.original.values, df_train.revised.values)\n",
    "val_data = EditDataset(df_val.original.values, df_val.revised.values)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "valloader = DataLoader(val_data, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZHl009HdFOes"
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SJoCGCKkY2E"
   },
   "source": [
    "Add missing tokens to t5-base tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "u8UUXVHDYu3P",
    "outputId": "5847be5d-4dc0-470e-8312-8d0b69000f6b"
   },
   "outputs": [],
   "source": [
    "tokenized_original = df.original.apply(lambda x: tokenizer.tokenize(x)).values\n",
    "tokenized_revised = df.revised.apply(lambda x: tokenizer.tokenize(x)).values\n",
    "\n",
    "length = max(map(len, tokenized_original + tokenized_revised))\n",
    "sents = np.array([xi+[None]*(length-len(xi)) for xi in tokenized_original + tokenized_revised])\n",
    "\n",
    "vocab = list(set(sents.reshape(-1)))\n",
    "tokenizer_vocab = tokenizer.get_vocab()\n",
    "\n",
    "missing = [token for token in vocab if token not in tokenizer_vocab]\n",
    "missing.remove(None)\n",
    "\n",
    "print(\"A total of {} missing tokens will be added.\".format(len(missing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Hz4U1QvDdBsa",
    "outputId": "747babd1-c5ec-4d56-d789-be975822f66b"
   },
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(missing)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAzI3PFydSUQ"
   },
   "source": [
    "Fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-tnQiCMPcoBE"
   },
   "outputs": [],
   "source": [
    "num_epochs = 4\n",
    "\n",
    "t_total = len(trainloader) // num_epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=t_total\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PWy5VAEdgRbk",
    "outputId": "cc6b6e1f-45e8-4a0f-bda1-185d97ed40f1"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "model.train() # Turn on the train mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  train(model, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "02AJhlgN7CFD",
    "outputId": "9b5a2cd2-883d-40ab-f663-75393b5f8197"
   },
   "outputs": [],
   "source": [
    "#os.mkdir(\"/content/drive/My Drive/Colab Notebooks/Pytorch/T5_editor/Edit_distance_30\")\n",
    "model.save_pretrained(\"editing_model/\")\n",
    "\n",
    "#os.mkdir(\"/content/drive/My Drive/Colab Notebooks/Pytorch/T5_editor/Edit_distance_30_tokenizer\")\n",
    "tokenizer.save_pretrained(\"editing_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Vwn7wePyw51e",
    "outputId": "eda01834-12e8-4391-8d3b-e6ad588350e5"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"edit: Cancer patients was used as a control groups.\", return_tensors=\"pt\")  # Batch size 1\n",
    "input_ids = input_ids.to(device)\n",
    "outputs = model.generate(input_ids)\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zsDZwHAEnF67"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"edit: The tropical cyclone’s (TC’s) development and movement relate to its structure and background environmental flow.\", return_tensors=\"pt\")  # Batch size 1\n",
    "input_ids = input_ids.to(device)\n",
    "outputs = model.generate(input_ids)\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9h-v28-Y1LCl"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"edit: The development and movement of the TCs relate to its structure and background environmental flow.\", return_tensors=\"pt\")  # Batch size 1\n",
    "input_ids = input_ids.to(device)\n",
    "outputs = model.generate(input_ids)\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03YelLFeo0QT"
   },
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"edit: A sentence with two error.\", return_tensors=\"pt\")  # Batch size 1\n",
    "input_ids = input_ids.to(device)\n",
    "outputs = model.generate(input_ids)\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSt-ZUdk_5nY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "T5 Encoder Decoder with Edit Distance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
