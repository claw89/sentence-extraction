{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for files that couldn't be written to the log because of unicode errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"D:\\\\proofread\"\n",
    "os.listdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlogged_files = []\n",
    "\n",
    "for file_name in os.listdir(base_dir):\n",
    "    with open(\"temp.txt\", \"at\") as file:\n",
    "        try:\n",
    "            file.write(file_name)\n",
    "        except:\n",
    "            unlogged_files.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlogged_ids = list(set([u[:13] for u in unlogged_files]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unlogged ids from sentences.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\")\n",
    "df = df[~df.file.isin(unlogged_ids)]\n",
    "df.to_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for files with incomplete sentence extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\buildSentenceDataset.log\", 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_lines = [l for l in lines if l.split(\":\")[0] == \"INFO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_cases = []\n",
    "for line in info_lines:\n",
    "    match = re.search(\"Added (\\d+) of (\\d+) sentences for (O-20\\d{2}-\\d{6})\", line)\n",
    "    if match is not None:\n",
    "        if int(match.group(1)) != int(match.group(2)):\n",
    "            incomplete_cases.append(match.group(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} cases with incomplete extraction out of {} total cases\".format(len(incomplete_cases), len(info_lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove incomplete cases from sentences.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\")\n",
    "df = df[~df.file.isin(incomplete_cases)]\n",
    "df.to_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build list of case ids to exclude from execution. If correct is set to true, cases with incomplete sentence extraction will be repeated. Otherwise, only cases not in the log will be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = True\n",
    "exclude_case_ids = []\n",
    "info_lines = [l for l in lines if l.split(\":\")[0] == \"INFO\"]\n",
    "\n",
    "for line in info_lines:\n",
    "    processed_match = re.search(\"O-20\\d{2}-\\d{6}\", line)\n",
    "    if processed_match is not None:\n",
    "        if correct:\n",
    "            correct_match = re.search(\"Added (\\d+) of (\\d+) sentences for (O-20\\d{2}-\\d{6})\", line)\n",
    "            if correct_match is not None:\n",
    "                if int(correct_match.group(1)) == int(correct_match.group(2)):\n",
    "                    exclude_case_ids.append(correct_match.group(3))\n",
    "        else:\n",
    "            exclude_case_ids.append(processed_match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove incomplete cases from sentences.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\")\n",
    "df = df[~df.file.isin(incomplete_cases)]\n",
    "df.to_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove certain case IDs from log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\buildSentenceDataset.log\", 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"long_sentence_files\", 'rb') as file:\n",
    "    long_sentence_files = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\buildSentenceDataset_temp.log\", 'w', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    for line in lines:\n",
    "        match = re.search(\"O-20\\d{2}-\\d{6}\", line)\n",
    "        if match is not None:\n",
    "            if match.group(0) not in long_sentence_files:\n",
    "                file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\")\n",
    "df = df[~df.file.isin(long_sentence_files)]\n",
    "df.to_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sentence_df = \n",
    "long_sentence_df[\"length\"] = long_sentence_df.original.str.len()\n",
    "long_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sentence_df.groupby('file').length.max().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import editdistance\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\")\n",
    "df = df.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.original.str.len().hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.original.str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sentences are excessively long (too long for the T5 tokenizer). These may be the files for which sentence tokenization failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df[[\"file\", \"original\"]].groupby(\"file\").count()\n",
    "counts[counts.original < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_ten = counts[counts.original < 10].index.values\n",
    "less_than_ten_df = df[df.file.isin(less_than_ten)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_ten_df[\"changes\"] = less_than_ten_df.astype(str).apply(lambda row: editdistance.eval(row.original, row.revised) / max(len(row.original), len(row.revised)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_ten_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sentence_files = list(counts[counts.original == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.file.isin(single_sentence_files)].original.str.len().hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.file.isin(single_sentence_files)].original.str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.file.isin(single_sentence_files)].original.str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this does not cover all long sentences. We will use the T5 tokenizer to determine the acceptable sentences directly (using Colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct sentence tokenization errors\n",
    "\n",
    "Sometimes, the spacy sentence tokenizer has made some errors.\n",
    "\n",
    "The following code is designed to correct for these sentence tokenization errors. Sentences for which the original and revised both start with a nonupper case letter are joined to the preceeding sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import editdistance\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_file = df.file.values[0]\n",
    "compressed = [previous_file]\n",
    "for file in df.file.values[1:]:\n",
    "    if file != previous_file:\n",
    "        if file not in compressed:\n",
    "            compressed.append(file)\n",
    "        else:\n",
    "            print(file)\n",
    "    previous_file = file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.file == \"O-2016-000006\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.original == df.loc[958016].original]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.file != \"o-2016-000255\"]\n",
    "df = df[~((df.index < 1120108) & (df.file == \"o-2016-006897\"))]\n",
    "df = df[~((df.index < 958768) & (df.file == \"O-2016-000006\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\")\n",
    "#Replace NAs with whitespaace or empty string\n",
    "#df = df.dropna()\n",
    "#df = df.fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the leading whitespace\n",
    "df[\"original\"] = df.original.apply(lambda x: x.strip(\"/\\r\"))\n",
    "df[\"revised\"] = df.revised.apply(lambda x: x.strip(\"/\\r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df.original.str.len() != 0]\n",
    "#df = df[df.revised.str.len() != 0]\n",
    "df = df[~((df.original.str.len() == 0) & (df.revised.str.len() == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:10000].file.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDO: use the average edit distance per file to remove files that had sentence misallignment during extraction\n",
    "\n",
    "Papers with a negative skewness in the distribution of extent of changes are likely to have been incorrectly extracted (i.e., the sentences are misaligned. They can be removed by check the skewnews of extent  of changes for all papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df.file == \"O-2016-002070\"]\n",
    "temp[\"changes\"] = temp.apply(lambda row: editdistance.eval(row.original, row.revised) / max(len(row.original), len(row.revised)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.changes.hist()\n",
    "plt.show()\n",
    "print(\"Skewness: \", temp.changes.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution of editdistance for the full dataset is too memory demanding for pandas. Instead run the calculation in dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extent_of_changes(row):\n",
    "    max_length = max(len(row.original), len(row.revised))\n",
    "    if max_length < 895:\n",
    "        return editdistance.eval(row.original, row.revised) / max_length\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"changes\"] = df.astype(str).apply(lambda row: get_extent_of_changes(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = df.groupby([\"file\"]).changes.skew()\n",
    "misalligned_files = skewness[skewness < -0.6].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.file.isin(misalligned_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences_for_tokenizer_correction.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine incorrectly divided sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences_for_tokenizer_correction.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_sentence_indexes = list(df[~df.astype(str).original.apply(lambda x: x[0].isupper()) & ~df.astype(str).revised.apply(lambda x: x[0].isupper())].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_sentence_indexes[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approx 16 min\n",
    "combine_index_lists = [[-1]]\n",
    "max_index = 0\n",
    "for i in tqdm(half_sentence_indexes):\n",
    "    if i > max_index:\n",
    "        index_list = [i-1]\n",
    "        while i in half_sentence_indexes:\n",
    "            index_list.append(i)\n",
    "            i += 1\n",
    "        combine_index_lists.append(index_list)\n",
    "        max_index = max(max_index, max(index_list))\n",
    "combine_index_lists.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_index_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of incorrect sentence tokenization is shown as follows. The spacy tokenizer mistaked the period following \"Fig\" as the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2196].original, df.loc[2196].revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2197].original, df.loc[2197].revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2198].original, df.loc[2198].revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2199].original, df.loc[2199].revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[2200].original, df.loc[2200].revised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the first character of the sencond sentence fragment, the two fragments should be joined either with a space or witout a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_space_join_1 = ['!', '%', ',', '-', '.', '/', ':', ';', '?', '_', '`', '|', '‐', '‒', '–', '—',]\n",
    "no_space_join_2 = ['\" ', \"' \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_fragments(fragment_list):\n",
    "    result = \"\"\n",
    "    for fragment in fragment_list:\n",
    "        if fragment[0] in no_space_join_1 or fragment[0:2] in no_space_join_2:\n",
    "            result += fragment\n",
    "        else:\n",
    "            result += \" \" + fragment\n",
    "    return result[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1478:1481]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_index_lists[1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approx 8 hours\n",
    "for index_list in tqdm(combine_index_lists):\n",
    "    start, stop = index_list[0], index_list[-1]\n",
    "    if start >= 0:\n",
    "        original = df.loc[start:stop].groupby(df.loc[start:stop][\"file\"])[\"original\"].transform(lambda x: join_fragments(x)).loc[start].replace(\" ,\", \",\")\n",
    "        revised = df.loc[start:stop].groupby(df.loc[start:stop][\"file\"])[\"revised\"].transform(lambda x: join_fragments(x)).loc[start].replace(\" ,\", \",\")\n",
    "\n",
    "        df.loc[start].original = original\n",
    "        df.loc[start].revised = revised\n",
    "\n",
    "        df = df.drop(index_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[812].original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Remove sentences that are too long for the T5 tokenizer (512 tokens) (send to Colab)\n",
    "#### TODO: After correcting tokenization errors, remove sentences where the original and revised are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.original != df.revised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame({\"A\": [1, 2, 3, 4], \"B\": [1, 2, 3, 4]})\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.drop([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences_for_tokenizer_correction.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"changes\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"changes\"] = df.astype(str).apply(lambda row: editdistance.eval(row.original, row.revised) / max(len(row.original), len(row.revised)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\Users\\\\Banjamin\\\\sentence-pair-extraction\\\\sentences_final.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
